{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772fb06a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-25T10:13:49.293959Z",
     "iopub.status.busy": "2024-05-25T10:13:49.293205Z",
     "iopub.status.idle": "2024-05-25T10:13:50.863268Z",
     "shell.execute_reply": "2024-05-25T10:13:50.862146Z"
    },
    "papermill": {
     "duration": 1.582817,
     "end_time": "2024-05-25T10:13:50.865572",
     "exception": false,
     "start_time": "2024-05-25T10:13:49.282755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bert_medical_records'...\r\n",
      "remote: Enumerating objects: 422, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (69/69), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (49/49), done.\u001b[K\r\n",
      "remote: Total 422 (delta 24), reused 38 (delta 16), pack-reused 353\u001b[K\r\n",
      "Receiving objects: 100% (422/422), 306.28 KiB | 6.96 MiB/s, done.\r\n",
      "Resolving deltas: 100% (251/251), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b ema https://github.com/edoppiap/bert_medical_records.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b01178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T10:13:50.883723Z",
     "iopub.status.busy": "2024-05-25T10:13:50.883383Z",
     "iopub.status.idle": "2024-05-25T10:14:30.456092Z",
     "shell.execute_reply": "2024-05-25T10:14:30.455156Z"
    },
    "papermill": {
     "duration": 39.584163,
     "end_time": "2024-05-25T10:14:30.458347",
     "exception": false,
     "start_time": "2024-05-25T10:13:50.874184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.15.0 (from -r bert_medical_records/requirements.txt (line 1))\r\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\r\n",
      "Collecting pandas==1.5.3 (from -r bert_medical_records/requirements.txt (line 2))\r\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting tokenizers==0.15.0 (from -r bert_medical_records/requirements.txt (line 3))\r\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Collecting transformers==4.35.2 (from -r bert_medical_records/requirements.txt (line 4))\r\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.20.1 in /opt/conda/lib/python3.10/site-packages (from -r bert_medical_records/requirements.txt (line 5)) (0.26.1)\r\n",
      "Collecting streamlit>=1.29.0 (from -r bert_medical_records/requirements.txt (line 6))\r\n",
      "  Downloading streamlit-1.35.0-py2.py3-none-any.whl.metadata (8.5 kB)\r\n",
      "Collecting stqdm (from -r bert_medical_records/requirements.txt (line 7))\r\n",
      "  Downloading stqdm-0.0.5-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r bert_medical_records/requirements.txt (line 8)) (2.1.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (1.24.4)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (11.0.0)\r\n",
      "Collecting pyarrow-hotfix (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1))\r\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (0.3.7)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (0.70.15)\r\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1))\r\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (3.9.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (0.20.3)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3->-r bert_medical_records/requirements.txt (line 2)) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3->-r bert_medical_records/requirements.txt (line 2)) (2023.3.post1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r bert_medical_records/requirements.txt (line 4)) (3.13.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r bert_medical_records/requirements.txt (line 4)) (2023.12.25)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r bert_medical_records/requirements.txt (line 4)) (0.4.2)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r bert_medical_records/requirements.txt (line 5)) (5.9.3)\r\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (5.2.0)\r\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (1.7.0)\r\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (4.2.4)\r\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (8.1.7)\r\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (9.5.0)\r\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (3.20.3)\r\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (13.7.0)\r\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (8.2.3)\r\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (0.10.2)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (4.9.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (3.1.41)\r\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6))\r\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (6.3.3)\r\n",
      "Collecting watchdog>=2.1.5 (from streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6))\r\n",
      "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r bert_medical_records/requirements.txt (line 8)) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r bert_medical_records/requirements.txt (line 8)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r bert_medical_records/requirements.txt (line 8)) (3.1.2)\r\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (4.20.0)\r\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (0.12.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (4.0.3)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (4.0.11)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (3.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r bert_medical_records/requirements.txt (line 8)) (2.1.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r bert_medical_records/requirements.txt (line 2)) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0->-r bert_medical_records/requirements.txt (line 1)) (2023.11.17)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (2.17.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r bert_medical_records/requirements.txt (line 8)) (1.3.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (5.0.1)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (0.32.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (0.16.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=1.29.0->-r bert_medical_records/requirements.txt (line 6)) (0.1.2)\r\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading streamlit-1.35.0-py2.py3-none-any.whl (8.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading stqdm-0.0.5-py3-none-any.whl (11 kB)\r\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\n",
      "Installing collected packages: watchdog, pyarrow-hotfix, fsspec, pydeck, pandas, tokenizers, transformers, datasets, streamlit, stqdm\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2023.12.2\r\n",
      "    Uninstalling fsspec-2023.12.2:\r\n",
      "      Successfully uninstalled fsspec-2023.12.2\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 2.1.4\r\n",
      "    Uninstalling pandas-2.1.4:\r\n",
      "      Successfully uninstalled pandas-2.1.4\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.15.1\r\n",
      "    Uninstalling tokenizers-0.15.1:\r\n",
      "      Successfully uninstalled tokenizers-0.15.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.37.0\r\n",
      "    Uninstalling transformers-4.37.0:\r\n",
      "      Successfully uninstalled transformers-4.37.0\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 2.1.0\r\n",
      "    Uninstalling datasets-2.1.0:\r\n",
      "      Successfully uninstalled datasets-2.1.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.0.11 which is incompatible.\r\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\r\n",
      "gcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "osmnx 1.8.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\r\n",
      "s3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tensorflowjs 4.16.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\r\n",
      "xarray 2024.1.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed datasets-2.15.0 fsspec-2023.10.0 pandas-1.5.3 pyarrow-hotfix-0.6 pydeck-0.9.1 stqdm-0.0.5 streamlit-1.35.0 tokenizers-0.15.0 transformers-4.35.2 watchdog-4.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r bert_medical_records/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f33651",
   "metadata": {
    "papermill": {
     "duration": 0.012668,
     "end_time": "2024-05-25T10:14:30.484531",
     "exception": false,
     "start_time": "2024-05-25T10:14:30.471863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create pre-train and finetuning dataset text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0f5821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T10:14:30.512922Z",
     "iopub.status.busy": "2024-05-25T10:14:30.512182Z",
     "iopub.status.idle": "2024-05-25T10:14:34.006244Z",
     "shell.execute_reply": "2024-05-25T10:14:34.004874Z"
    },
    "papermill": {
     "duration": 3.5127,
     "end_time": "2024-05-25T10:14:34.009947",
     "exception": false,
     "start_time": "2024-05-25T10:14:30.497247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing text file from csv dataset: 100%|██| 680/680 [00:01<00:00, 595.57it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/preprocessing_python/text_generator.py \\\n",
    "        --file_path /kaggle/input/new-base-data/PHeP_simulated_data.csv \\\n",
    "        --output_folder /kaggle/working \\\n",
    "        --output_name dataset.txt \\\n",
    "        --create_pretrain_text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8d8b9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T10:14:34.052062Z",
     "iopub.status.busy": "2024-05-25T10:14:34.051711Z",
     "iopub.status.idle": "2024-05-25T10:14:35.947720Z",
     "shell.execute_reply": "2024-05-25T10:14:35.946619Z"
    },
    "papermill": {
     "duration": 1.91968,
     "end_time": "2024-05-25T10:14:35.950031",
     "exception": false,
     "start_time": "2024-05-25T10:14:34.030351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating nsp dataset: 10811it [00:00, 801935.14it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/preprocessing_python/text_generator.py \\\n",
    "        --file_path /kaggle/working/dataset.txt \\\n",
    "        --output_folder /kaggle/working \\\n",
    "        --output_name nsp_dataset.txt \\\n",
    "        --create_nsp_text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94a1870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T10:14:35.980105Z",
     "iopub.status.busy": "2024-05-25T10:14:35.979759Z",
     "iopub.status.idle": "2024-05-25T10:14:39.376562Z",
     "shell.execute_reply": "2024-05-25T10:14:39.375460Z"
    },
    "papermill": {
     "duration": 3.414585,
     "end_time": "2024-05-25T10:14:39.379043",
     "exception": false,
     "start_time": "2024-05-25T10:14:35.964458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input file: 100%|████████████████████| 680/680 [00:01<00:00, 490.20it/s]\r\n",
      "Creating dataset for finetuning: 11481it [00:00, 152100.78it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/preprocessing_python/text_generator.py \\\n",
    "        --file_path /kaggle/input/new-base-data/PHeP_simulated_data.csv \\\n",
    "        --output_folder /kaggle/working \\\n",
    "        --output_name finetuning_dataset.txt \\\n",
    "        --create_finetuning_text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442be7df",
   "metadata": {
    "papermill": {
     "duration": 0.01498,
     "end_time": "2024-05-25T10:14:39.409571",
     "exception": false,
     "start_time": "2024-05-25T10:14:39.394591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Both mlm_nsp train and eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207e742",
   "metadata": {
    "papermill": {
     "duration": 0.014855,
     "end_time": "2024-05-25T10:14:39.439635",
     "exception": false,
     "start_time": "2024-05-25T10:14:39.424780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Only our pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd59117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T10:14:39.471281Z",
     "iopub.status.busy": "2024-05-25T10:14:39.470967Z",
     "iopub.status.idle": "2024-05-25T11:36:20.563957Z",
     "shell.execute_reply": "2024-05-25T11:36:20.562838Z"
    },
    "papermill": {
     "duration": 4901.111785,
     "end_time": "2024-05-25T11:36:20.566485",
     "exception": false,
     "start_time": "2024-05-25T10:14:39.454700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 10:14:49.739905: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 10:14:49.740004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 10:14:49.869942: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/our_pretrain_only_mlm_nsp\r\n",
      "Train the tokenizer\r\n",
      "\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 855      /      855\r\n",
      "\u001b[2K[00:00:00] Count pairs                    ██████████████████ 855      /      855\r\n",
      "\u001b[2K[00:00:00] Compute merges                 ██████████████████ 1243     /     1243\r\n",
      "Saving the tokenizer\r\n",
      "Loading the workable tokenizer\r\n",
      "  0%|                                                   | 0/865 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:147: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['next_sentence_label'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████| 865/865 [09:48<00:00,  1.47it/s, loss=0.736]\r\n",
      "Epoch 1: 100%|████████████████████| 865/865 [09:48<00:00,  1.47it/s, loss=0.739]\r\n",
      "Epoch 2: 100%|████████████████████| 865/865 [09:49<00:00,  1.47it/s, loss=0.701]\r\n",
      "Epoch 3: 100%|████████████████████| 865/865 [09:49<00:00,  1.47it/s, loss=0.863]\r\n",
      "Epoch 4: 100%|████████████████████| 865/865 [09:49<00:00,  1.47it/s, loss=0.754]\r\n",
      "Epoch 5: 100%|████████████████████| 865/865 [09:49<00:00,  1.47it/s, loss=0.135]\r\n",
      "Epoch 6: 100%|████████████████████| 865/865 [09:49<00:00,  1.47it/s, loss=0.296]\r\n",
      "Epoch 7: 100%|█████████████████████| 865/865 [09:49<00:00,  1.47it/s, loss=1.04]\r\n",
      "Average loss = 1.0381766557693481\r\n",
      "Evaluating: 100%|█████████████████| 217/217 [02:42<00:00,  1.34it/s, loss=0.757]\r\n",
      "Nsp acc: 0.8538390379278445\r\n",
      "Mlm acc: 0.03048780487804878\r\n"
     ]
    }
   ],
   "source": [
    "!python bert_medical_records/run_pre_train.py \\\n",
    "      --do_eval \\\n",
    "      --do_train \\\n",
    "      --train_batch_size 10 \\\n",
    "      --pre_train_tasks mlm_nsp \\\n",
    "      --num_epochs 8 \\\n",
    "      --input_file /kaggle/working/nsp_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/our_pretrain_only_mlm_nsp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7fc7097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:36:22.815082Z",
     "iopub.status.busy": "2024-05-25T11:36:22.814100Z",
     "iopub.status.idle": "2024-05-25T11:45:51.686316Z",
     "shell.execute_reply": "2024-05-25T11:45:51.685207Z"
    },
    "papermill": {
     "duration": 570.028639,
     "end_time": "2024-05-25T11:45:51.688633",
     "exception": false,
     "start_time": "2024-05-25T11:36:21.659994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 11:36:28.465995: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 11:36:28.466049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 11:36:28.467557: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/our_pretrain_only_mlm_nsp\r\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/output/our_pretrain_only_mlm_nsp/pre_trained_model and are newly initialized: ['classifier.bias', 'classifier.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                   | 0/919 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:105: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['labels'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████| 919/919 [08:32<00:00,  1.79it/s, loss=0.744]\r\n",
      "Average loss = 0.7441616654396057\r\n",
      "Evaluating: 100%|███████████████████| 230/230 [00:44<00:00,  5.17it/s, loss=145]\r\n",
      "1631\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "result = {'acc': 0.7103658536585366, 'f1': 0.8260528380852733, 'acc_and_f1': 0.7682093458719049}\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/run_glue.py \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --num_epochs 1 \\\n",
    "      --model_input /kaggle/working/output/our_pretrain_only_mlm_nsp \\\n",
    "      --input_file /kaggle/working/finetuning_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/our_pretrain_only_mlm_nsp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa06ee",
   "metadata": {
    "papermill": {
     "duration": 1.370253,
     "end_time": "2024-05-25T11:45:54.340819",
     "exception": false,
     "start_time": "2024-05-25T11:45:52.970566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bert pretrained without further pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b7aa477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:45:56.998860Z",
     "iopub.status.busy": "2024-05-25T11:45:56.998444Z",
     "iopub.status.idle": "2024-05-25T11:49:39.690062Z",
     "shell.execute_reply": "2024-05-25T11:49:39.688874Z"
    },
    "papermill": {
     "duration": 223.978699,
     "end_time": "2024-05-25T11:49:39.692625",
     "exception": false,
     "start_time": "2024-05-25T11:45:55.713926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 11:46:02.709980: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 11:46:02.710046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 11:46:02.711630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "tokenizer_config.json: 100%|██████████████████| 48.0/48.0 [00:00<00:00, 209kB/s]\r\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 11.5MB/s]\r\n",
      "tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 7.33MB/s]\r\n",
      "config.json: 100%|█████████████████████████████| 570/570 [00:00<00:00, 2.96MB/s]\r\n",
      "model.safetensors: 100%|██████████████████████| 440M/440M [00:01<00:00, 332MB/s]\r\n",
      "Evaluating:   0%|                                       | 0/217 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:147: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['next_sentence_label'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Evaluating: 100%|██████████████████| 217/217 [03:28<00:00,  1.04it/s, loss=23.9]\r\n",
      "Nsp acc: 0.5185013876040703\r\n",
      "Mlm acc: 0.26649653663871675\r\n"
     ]
    }
   ],
   "source": [
    "!python bert_medical_records/run_pre_train.py \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --pre_train_tasks mlm_nsp \\\n",
    "      --input_file /kaggle/working/nsp_dataset.txt \\\n",
    "      --use_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef774850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:49:42.448042Z",
     "iopub.status.busy": "2024-05-25T11:49:42.447644Z",
     "iopub.status.idle": "2024-05-25T11:59:12.640722Z",
     "shell.execute_reply": "2024-05-25T11:59:12.639352Z"
    },
    "papermill": {
     "duration": 571.57228,
     "end_time": "2024-05-25T11:59:12.643285",
     "exception": false,
     "start_time": "2024-05-25T11:49:41.071005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 11:49:48.233845: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 11:49:48.233921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 11:49:48.235441: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/just_bert_mlm_nsp\r\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                   | 0/919 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:105: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['labels'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████| 919/919 [08:33<00:00,  1.79it/s, loss=0.693]\r\n",
      "Average loss = 0.6926518678665161\r\n",
      "Evaluating: 100%|███████████████████| 230/230 [00:44<00:00,  5.17it/s, loss=141]\r\n",
      "1599\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "result = {'acc': 0.6964285714285714, 'f1': 0.8210526315789474, 'acc_and_f1': 0.7587406015037594}\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/run_glue.py \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --num_epochs 1 \\\n",
    "      --input_file /kaggle/working/finetuning_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/just_bert_mlm_nsp \\\n",
    "      --use_pretrained_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a03e79",
   "metadata": {
    "papermill": {
     "duration": 1.484091,
     "end_time": "2024-05-25T11:59:15.732706",
     "exception": false,
     "start_time": "2024-05-25T11:59:14.248615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bert pretrained with our pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4efdea7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:59:18.761506Z",
     "iopub.status.busy": "2024-05-25T11:59:18.761096Z",
     "iopub.status.idle": "2024-05-25T13:32:49.989533Z",
     "shell.execute_reply": "2024-05-25T13:32:49.988394Z"
    },
    "papermill": {
     "duration": 5612.716555,
     "end_time": "2024-05-25T13:32:49.991828",
     "exception": false,
     "start_time": "2024-05-25T11:59:17.275273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 11:59:24.655845: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 11:59:24.655914: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 11:59:24.657458: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/bert_mlm_nsp_with_pretrain\r\n",
      "  0%|                                                   | 0/865 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:147: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['next_sentence_label'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████| 865/865 [11:14<00:00,  1.28it/s, loss=0.743]\r\n",
      "Epoch 1: 100%|████████████████████| 865/865 [11:18<00:00,  1.27it/s, loss=0.461]\r\n",
      "Epoch 2: 100%|████████████████████| 865/865 [11:25<00:00,  1.26it/s, loss=0.175]\r\n",
      "Epoch 3: 100%|████████████████████| 865/865 [11:14<00:00,  1.28it/s, loss=0.182]\r\n",
      "Epoch 4: 100%|███████████████████| 865/865 [11:09<00:00,  1.29it/s, loss=0.0174]\r\n",
      "Epoch 5: 100%|███████████████████| 865/865 [11:09<00:00,  1.29it/s, loss=0.0577]\r\n",
      "Epoch 6: 100%|███████████████████| 865/865 [11:11<00:00,  1.29it/s, loss=0.0155]\r\n",
      "Epoch 7: 100%|██████████████████| 865/865 [11:11<00:00,  1.29it/s, loss=0.00698]\r\n",
      "Average loss = 0.006978851277381182\r\n",
      "Evaluating: 100%|███████████████| 217/217 [03:22<00:00,  1.07it/s, loss=0.00091]\r\n",
      "Nsp acc: 0.9597594819611471\r\n",
      "Mlm acc: 0.49327354260089684\r\n"
     ]
    }
   ],
   "source": [
    "!python bert_medical_records/run_pre_train.py \\\n",
    "      --do_eval \\\n",
    "      --do_train \\\n",
    "      --train_batch_size 10 \\\n",
    "      --pre_train_tasks mlm_nsp \\\n",
    "      --num_epochs 8 \\\n",
    "      --input_file /kaggle/working/nsp_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/bert_mlm_nsp_with_pretrain \\\n",
    "      --use_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cccc9317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:32:55.382769Z",
     "iopub.status.busy": "2024-05-25T13:32:55.382372Z",
     "iopub.status.idle": "2024-05-25T13:42:23.896387Z",
     "shell.execute_reply": "2024-05-25T13:42:23.895233Z"
    },
    "papermill": {
     "duration": 571.313181,
     "end_time": "2024-05-25T13:42:23.899001",
     "exception": false,
     "start_time": "2024-05-25T13:32:52.585820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 13:33:00.971215: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 13:33:00.971275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 13:33:00.972930: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/bert_mlm_nsp_with_pretrain\r\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/output/bert_mlm_nsp_with_pretrain/pre_trained_model and are newly initialized: ['classifier.weight', 'classifier.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                   | 0/919 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:105: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['labels'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████| 919/919 [08:32<00:00,  1.79it/s, loss=0.144]\r\n",
      "Average loss = 0.14430633187294006\r\n",
      "Evaluating: 100%|██████████████████| 230/230 [00:44<00:00,  5.19it/s, loss=48.8]\r\n",
      "2038\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "result = {'acc': 0.8876306620209059, 'f1': 0.9164507772020726, 'acc_and_f1': 0.9020407196114892}\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/run_glue.py \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --num_epochs 1 \\\n",
    "      --model_input /kaggle/working/output/bert_mlm_nsp_with_pretrain \\\n",
    "      --input_file /kaggle/working/finetuning_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/bert_mlm_nsp_with_pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135dba0",
   "metadata": {
    "papermill": {
     "duration": 2.845112,
     "end_time": "2024-05-25T13:42:29.640047",
     "exception": false,
     "start_time": "2024-05-25T13:42:26.794935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Only mlm pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f36471",
   "metadata": {
    "papermill": {
     "duration": 2.91327,
     "end_time": "2024-05-25T13:42:35.391708",
     "exception": false,
     "start_time": "2024-05-25T13:42:32.478438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Only our pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1ca720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:42:41.182071Z",
     "iopub.status.busy": "2024-05-25T13:42:41.181152Z",
     "iopub.status.idle": "2024-05-25T13:48:02.333273Z",
     "shell.execute_reply": "2024-05-25T13:48:02.331856Z"
    },
    "papermill": {
     "duration": 324.118429,
     "end_time": "2024-05-25T13:48:02.335438",
     "exception": false,
     "start_time": "2024-05-25T13:42:38.217009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 13:42:46.751754: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 13:42:46.751807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 13:42:46.753244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/our_pretrain_only_mlm\r\n",
      "Train the tokenizer\r\n",
      "\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 854      /      854\r\n",
      "\u001b[2K[00:00:00] Count pairs                    ██████████████████ 854      /      854\r\n",
      "\u001b[2K[00:00:00] Compute merges                 ██████████████████ 1248     /     1248\r\n",
      "Saving the tokenizer\r\n",
      "Loading the workable tokenizer\r\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████████| 55/55 [00:37<00:00,  1.47it/s, loss=6.8]\r\n",
      "Epoch 1: 100%|███████████████████████| 55/55 [00:37<00:00,  1.48it/s, loss=3.14]\r\n",
      "Epoch 2: 100%|██████████████████████| 55/55 [00:37<00:00,  1.48it/s, loss=0.721]\r\n",
      "Epoch 3: 100%|██████████████████████| 55/55 [00:37<00:00,  1.48it/s, loss=0.324]\r\n",
      "Epoch 4: 100%|██████████████████████| 55/55 [00:37<00:00,  1.48it/s, loss=0.153]\r\n",
      "Epoch 5: 100%|██████████████████████| 55/55 [00:37<00:00,  1.48it/s, loss=0.262]\r\n",
      "Epoch 6: 100%|██████████████████████| 55/55 [00:37<00:00,  1.48it/s, loss=0.158]\r\n",
      "Epoch 7: 100%|██████████████████████| 55/55 [00:37<00:00,  1.48it/s, loss=0.194]\r\n",
      "Average loss = 0.1941179633140564\r\n",
      "Evaluating:   0%|                                        | 0/14 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Evaluating: 100%|███████████████████| 14/14 [00:10<00:00,  1.28it/s, loss=0.507]\r\n",
      "Mlm acc: 0.13667117726657646\r\n"
     ]
    }
   ],
   "source": [
    "!python bert_medical_records/run_pre_train.py \\\n",
    "      --do_eval \\\n",
    "      --do_train \\\n",
    "      --train_batch_size 10 \\\n",
    "      --pre_train_tasks mlm \\\n",
    "      --num_epochs 8 \\\n",
    "      --input_file /kaggle/working/dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/our_pretrain_only_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d0f06e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:48:08.267061Z",
     "iopub.status.busy": "2024-05-25T13:48:08.266611Z",
     "iopub.status.idle": "2024-05-25T13:57:36.146195Z",
     "shell.execute_reply": "2024-05-25T13:57:36.145207Z"
    },
    "papermill": {
     "duration": 570.812812,
     "end_time": "2024-05-25T13:57:36.148346",
     "exception": false,
     "start_time": "2024-05-25T13:48:05.335534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 13:48:14.024125: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 13:48:14.024191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 13:48:14.025848: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/our_pretrain_only_mlm\r\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/output/our_pretrain_only_mlm/pre_trained_model and are newly initialized: ['bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                   | 0/919 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:105: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['labels'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|███████████████████| 919/919 [08:31<00:00,  1.80it/s, loss=0.0114]\r\n",
      "Average loss = 0.011443858966231346\r\n",
      "Evaluating: 100%|██████████████████| 230/230 [00:44<00:00,  5.19it/s, loss=84.8]\r\n",
      "1980\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "result = {'acc': 0.8623693379790941, 'f1': 0.9031269160024525, 'acc_and_f1': 0.8827481269907733}\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/run_glue.py \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --num_epochs 1 \\\n",
    "      --model_input /kaggle/working/output/our_pretrain_only_mlm \\\n",
    "      --input_file /kaggle/working/finetuning_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/our_pretrain_only_mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca1cfc",
   "metadata": {
    "papermill": {
     "duration": 3.063517,
     "end_time": "2024-05-25T13:57:42.233356",
     "exception": false,
     "start_time": "2024-05-25T13:57:39.169839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bert pretrained without further pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a86b7d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:57:48.463692Z",
     "iopub.status.busy": "2024-05-25T13:57:48.463290Z",
     "iopub.status.idle": "2024-05-25T13:58:13.158620Z",
     "shell.execute_reply": "2024-05-25T13:58:13.157732Z"
    },
    "papermill": {
     "duration": 27.790994,
     "end_time": "2024-05-25T13:58:13.160988",
     "exception": false,
     "start_time": "2024-05-25T13:57:45.369994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 13:57:53.975318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 13:57:53.975373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 13:57:53.977022: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\r\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "Evaluating:   0%|                                        | 0/14 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Evaluating: 100%|████████████████████| 14/14 [00:12<00:00,  1.08it/s, loss=8.91]\r\n",
      "Mlm acc: 0.5589899869394863\r\n"
     ]
    }
   ],
   "source": [
    "!python bert_medical_records/run_pre_train.py \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --pre_train_tasks mlm \\\n",
    "      --input_file /kaggle/working/dataset.txt \\\n",
    "      --use_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42fb3e92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:58:19.371627Z",
     "iopub.status.busy": "2024-05-25T13:58:19.370794Z",
     "iopub.status.idle": "2024-05-25T14:07:46.956256Z",
     "shell.execute_reply": "2024-05-25T14:07:46.955119Z"
    },
    "papermill": {
     "duration": 571.295947,
     "end_time": "2024-05-25T14:07:47.549258",
     "exception": false,
     "start_time": "2024-05-25T13:58:16.253311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 13:58:24.834083: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 13:58:24.834138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 13:58:24.835546: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/just_bert_mlm_only\r\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                   | 0/919 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:105: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['labels'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████| 919/919 [08:31<00:00,  1.80it/s, loss=0.681]\r\n",
      "Average loss = 0.6809185743331909\r\n",
      "Evaluating: 100%|████████████████████| 230/230 [00:44<00:00,  5.20it/s, loss=76]\r\n",
      "1852\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "result = {'acc': 0.8066202090592335, 'f1': 0.8737919272313814, 'acc_and_f1': 0.8402060681453074}\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/run_glue.py \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --num_epochs 1 \\\n",
    "      --input_file /kaggle/working/finetuning_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/just_bert_mlm_only \\\n",
    "      --use_pretrained_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3b9ef",
   "metadata": {
    "papermill": {
     "duration": 3.443432,
     "end_time": "2024-05-25T14:07:54.230184",
     "exception": false,
     "start_time": "2024-05-25T14:07:50.786752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bert pretrained with our pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff72ea7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:08:00.648767Z",
     "iopub.status.busy": "2024-05-25T14:08:00.648072Z",
     "iopub.status.idle": "2024-05-25T14:14:15.866415Z",
     "shell.execute_reply": "2024-05-25T14:14:15.865471Z"
    },
    "papermill": {
     "duration": 378.519825,
     "end_time": "2024-05-25T14:14:15.868761",
     "exception": false,
     "start_time": "2024-05-25T14:07:57.348936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 14:08:06.119274: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 14:08:06.119337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 14:08:06.120837: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/bert_only_mlm_with_pretrain\r\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\r\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|██████████████████████| 55/55 [00:43<00:00,  1.25it/s, loss=0.178]\r\n",
      "Epoch 1: 100%|█████████████████████| 55/55 [00:43<00:00,  1.25it/s, loss=0.0855]\r\n",
      "Epoch 2: 100%|████████████████████| 55/55 [00:43<00:00,  1.26it/s, loss=0.00988]\r\n",
      "Epoch 3: 100%|█████████████████████| 55/55 [00:43<00:00,  1.26it/s, loss=0.0113]\r\n",
      "Epoch 4: 100%|████████████████████| 55/55 [00:43<00:00,  1.26it/s, loss=0.00664]\r\n",
      "Epoch 5: 100%|█████████████████████| 55/55 [00:43<00:00,  1.26it/s, loss=0.0159]\r\n",
      "Epoch 6: 100%|████████████████████| 55/55 [00:43<00:00,  1.26it/s, loss=0.00457]\r\n",
      "Epoch 7: 100%|█████████████████████| 55/55 [00:43<00:00,  1.26it/s, loss=0.0026]\r\n",
      "Average loss = 0.0026046799030154943\r\n",
      "Evaluating:   0%|                                        | 0/14 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Evaluating: 100%|█████████████████| 14/14 [00:13<00:00,  1.07it/s, loss=0.00225]\r\n",
      "Mlm acc: 0.9784244856999498\r\n"
     ]
    }
   ],
   "source": [
    "!python bert_medical_records/run_pre_train.py \\\n",
    "      --do_eval \\\n",
    "      --do_train \\\n",
    "      --train_batch_size 10 \\\n",
    "      --pre_train_tasks mlm \\\n",
    "      --num_epochs 8 \\\n",
    "      --input_file /kaggle/working/dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/bert_only_mlm_with_pretrain \\\n",
    "      --use_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f10abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:14:22.456010Z",
     "iopub.status.busy": "2024-05-25T14:14:22.455271Z",
     "iopub.status.idle": "2024-05-25T14:23:50.260152Z",
     "shell.execute_reply": "2024-05-25T14:23:50.258908Z"
    },
    "papermill": {
     "duration": 570.991111,
     "end_time": "2024-05-25T14:23:50.262646",
     "exception": false,
     "start_time": "2024-05-25T14:14:19.271535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 14:14:27.868501: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 14:14:27.868555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 14:14:27.869980: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/bert_only_mlm_with_pretrain\r\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/output/bert_only_mlm_with_pretrain/pre_trained_model and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                   | 0/919 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:105: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['labels'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|███████████████████| 919/919 [08:31<00:00,  1.80it/s, loss=0.0341]\r\n",
      "Average loss = 0.03409164026379585\r\n",
      "Evaluating: 100%|██████████████████| 230/230 [00:44<00:00,  5.19it/s, loss=56.3]\r\n",
      "2034\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "result = {'acc': 0.8858885017421603, 'f1': 0.9139290407358739, 'acc_and_f1': 0.899908771239017}\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/run_glue.py \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --num_epochs 1 \\\n",
    "      --model_input /kaggle/working/output/bert_only_mlm_with_pretrain \\\n",
    "      --input_file /kaggle/working/finetuning_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/bert_only_mlm_with_pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd683c94",
   "metadata": {
    "papermill": {
     "duration": 3.661567,
     "end_time": "2024-05-25T14:23:57.358488",
     "exception": false,
     "start_time": "2024-05-25T14:23:53.696921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Only NSP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb316d1",
   "metadata": {
    "papermill": {
     "duration": 3.550286,
     "end_time": "2024-05-25T14:24:04.274543",
     "exception": false,
     "start_time": "2024-05-25T14:24:00.724257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Only our pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f50265db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:24:11.204938Z",
     "iopub.status.busy": "2024-05-25T14:24:11.204182Z",
     "iopub.status.idle": "2024-05-25T15:29:54.291391Z",
     "shell.execute_reply": "2024-05-25T15:29:54.290418Z"
    },
    "papermill": {
     "duration": 3946.446147,
     "end_time": "2024-05-25T15:29:54.293641",
     "exception": false,
     "start_time": "2024-05-25T14:24:07.847494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 14:24:16.754513: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 14:24:16.754579: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 14:24:16.756090: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/our_pretrain_only_nsp\r\n",
      "Train the tokenizer\r\n",
      "\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 855      /      855\r\n",
      "\u001b[2K[00:00:00] Count pairs                    ██████████████████ 855      /      855\r\n",
      "\u001b[2K[00:00:00] Compute merges                 ██████████████████ 1243     /     1243\r\n",
      "Saving the tokenizer\r\n",
      "Loading the workable tokenizer\r\n",
      "  0%|                                                   | 0/865 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:147: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['next_sentence_label'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1470: FutureWarning: The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\r\n",
      "  warnings.warn(\r\n",
      "Epoch 0: 100%|████████████████████| 865/865 [08:05<00:00,  1.78it/s, loss=0.687]\r\n",
      "Epoch 1: 100%|████████████████████| 865/865 [08:05<00:00,  1.78it/s, loss=0.721]\r\n",
      "Epoch 2: 100%|████████████████████| 865/865 [08:06<00:00,  1.78it/s, loss=0.694]\r\n",
      "Epoch 3: 100%|████████████████████| 865/865 [08:02<00:00,  1.79it/s, loss=0.662]\r\n",
      "Epoch 4: 100%|████████████████████| 865/865 [08:06<00:00,  1.78it/s, loss=0.685]\r\n",
      "Epoch 5: 100%|████████████████████| 865/865 [08:06<00:00,  1.78it/s, loss=0.651]\r\n",
      "Epoch 6: 100%|████████████████████| 865/865 [08:06<00:00,  1.78it/s, loss=0.688]\r\n",
      "Epoch 7: 100%|█████████████████████| 865/865 [08:06<00:00,  1.78it/s, loss=0.71]\r\n",
      "Average loss = 0.7095350623130798\r\n",
      "Evaluating: 100%|█████████████████| 217/217 [00:42<00:00,  5.05it/s, loss=0.693]\r\n",
      "Nsp acc: 0.503700277520814\r\n"
     ]
    }
   ],
   "source": [
    "!python bert_medical_records/run_pre_train.py \\\n",
    "      --do_eval \\\n",
    "      --do_train \\\n",
    "      --train_batch_size 10 \\\n",
    "      --pre_train_tasks nsp \\\n",
    "      --num_epochs 8 \\\n",
    "      --input_file /kaggle/working/nsp_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/our_pretrain_only_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20d34274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:30:03.625166Z",
     "iopub.status.busy": "2024-05-25T15:30:03.623495Z",
     "iopub.status.idle": "2024-05-25T15:39:30.756623Z",
     "shell.execute_reply": "2024-05-25T15:39:30.755650Z"
    },
    "papermill": {
     "duration": 571.796203,
     "end_time": "2024-05-25T15:39:30.758944",
     "exception": false,
     "start_time": "2024-05-25T15:29:58.962741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 15:30:09.242577: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 15:30:09.242639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 15:30:09.244096: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/our_pretrain_only_nsp\r\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/output/our_pretrain_only_nsp/pre_trained_model and are newly initialized: ['classifier.weight', 'classifier.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                   | 0/919 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:105: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['labels'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████| 919/919 [08:31<00:00,  1.80it/s, loss=0.522]\r\n",
      "Average loss = 0.5219243764877319\r\n",
      "Evaluating: 100%|███████████████████| 230/230 [00:44<00:00,  5.22it/s, loss=140]\r\n",
      "1626\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "result = {'acc': 0.7081881533101045, 'f1': 0.8291687914329424, 'acc_and_f1': 0.7686784723715234}\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/run_glue.py \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --num_epochs 1 \\\n",
    "      --model_input /kaggle/working/output/our_pretrain_only_nsp \\\n",
    "      --input_file /kaggle/working/finetuning_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/our_pretrain_only_nsp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d5032",
   "metadata": {
    "papermill": {
     "duration": 4.831187,
     "end_time": "2024-05-25T15:39:40.384697",
     "exception": false,
     "start_time": "2024-05-25T15:39:35.553510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bert pretrained with no further pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba6dc107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:39:49.958576Z",
     "iopub.status.busy": "2024-05-25T15:39:49.958204Z",
     "iopub.status.idle": "2024-05-25T15:41:03.096144Z",
     "shell.execute_reply": "2024-05-25T15:41:03.095002Z"
    },
    "papermill": {
     "duration": 77.91327,
     "end_time": "2024-05-25T15:41:03.098614",
     "exception": false,
     "start_time": "2024-05-25T15:39:45.185344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 15:39:55.459036: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 15:39:55.459093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 15:39:55.460551: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Evaluating:   0%|                                       | 0/217 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:147: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['next_sentence_label'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1470: FutureWarning: The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\r\n",
      "  warnings.warn(\r\n",
      "Evaluating: 100%|██████████████████| 217/217 [01:02<00:00,  3.49it/s, loss=5.59]\r\n",
      "Nsp acc: 0.5004625346901017\r\n"
     ]
    }
   ],
   "source": [
    "!python bert_medical_records/run_pre_train.py \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --pre_train_tasks nsp \\\n",
    "      --input_file /kaggle/working/nsp_dataset.txt \\\n",
    "      --use_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "365defbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:41:12.865396Z",
     "iopub.status.busy": "2024-05-25T15:41:12.865023Z",
     "iopub.status.idle": "2024-05-25T15:50:40.692674Z",
     "shell.execute_reply": "2024-05-25T15:50:40.691742Z"
    },
    "papermill": {
     "duration": 572.711437,
     "end_time": "2024-05-25T15:50:40.695252",
     "exception": false,
     "start_time": "2024-05-25T15:41:07.983815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 15:41:18.330989: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 15:41:18.331047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 15:41:18.332530: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/just_bert_only_nsp\r\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                   | 0/919 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:105: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['labels'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████| 919/919 [08:31<00:00,  1.80it/s, loss=0.473]\r\n",
      "Average loss = 0.47298312187194824\r\n",
      "Evaluating: 100%|███████████████████| 230/230 [00:44<00:00,  5.21it/s, loss=146]\r\n",
      "1543\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "result = {'acc': 0.6720383275261324, 'f1': 0.8038551706173482, 'acc_and_f1': 0.7379467490717403}\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/run_glue.py \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --num_epochs 1 \\\n",
    "      --input_file /kaggle/working/finetuning_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/just_bert_only_nsp \\\n",
    "      --use_pretrained_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b80788",
   "metadata": {
    "papermill": {
     "duration": 4.92326,
     "end_time": "2024-05-25T15:50:50.739274",
     "exception": false,
     "start_time": "2024-05-25T15:50:45.816014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bert pretrained with our pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1de6834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:51:00.603267Z",
     "iopub.status.busy": "2024-05-25T15:51:00.602910Z",
     "iopub.status.idle": "2024-05-25T16:56:47.566739Z",
     "shell.execute_reply": "2024-05-25T16:56:47.565733Z"
    },
    "papermill": {
     "duration": 3951.891692,
     "end_time": "2024-05-25T16:56:47.569131",
     "exception": false,
     "start_time": "2024-05-25T15:50:55.677439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 15:51:06.100553: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 15:51:06.100604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 15:51:06.102131: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/bert_nsp_with_pretrain\r\n",
      "Train the tokenizer\r\n",
      "\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 855      /      855\r\n",
      "\u001b[2K[00:00:00] Count pairs                    ██████████████████ 855      /      855\r\n",
      "\u001b[2K[00:00:00] Compute merges                 ██████████████████ 1243     /     1243\r\n",
      "Saving the tokenizer\r\n",
      "Loading the workable tokenizer\r\n",
      "  0%|                                                   | 0/865 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:147: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['next_sentence_label'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1470: FutureWarning: The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\r\n",
      "  warnings.warn(\r\n",
      "Epoch 0: 100%|████████████████████| 865/865 [08:05<00:00,  1.78it/s, loss=0.722]\r\n",
      "Epoch 1: 100%|████████████████████| 865/865 [08:05<00:00,  1.78it/s, loss=0.706]\r\n",
      "Epoch 2: 100%|████████████████████| 865/865 [08:05<00:00,  1.78it/s, loss=0.693]\r\n",
      "Epoch 3: 100%|████████████████████| 865/865 [08:06<00:00,  1.78it/s, loss=0.688]\r\n",
      "Epoch 4: 100%|████████████████████| 865/865 [08:07<00:00,  1.77it/s, loss=0.681]\r\n",
      "Epoch 5: 100%|████████████████████| 865/865 [08:07<00:00,  1.78it/s, loss=0.692]\r\n",
      "Epoch 6: 100%|████████████████████| 865/865 [08:06<00:00,  1.78it/s, loss=0.684]\r\n",
      "Epoch 7: 100%|████████████████████| 865/865 [08:06<00:00,  1.78it/s, loss=0.695]\r\n",
      "Average loss = 0.6948276162147522\r\n",
      "Evaluating: 100%|█████████████████| 217/217 [00:42<00:00,  5.07it/s, loss=0.707]\r\n",
      "Nsp acc: 0.5069380203515264\r\n"
     ]
    }
   ],
   "source": [
    "!python bert_medical_records/run_pre_train.py \\\n",
    "      --do_eval \\\n",
    "      --do_train \\\n",
    "      --train_batch_size 10 \\\n",
    "      --pre_train_tasks nsp \\\n",
    "      --num_epochs 8 \\\n",
    "      --input_file /kaggle/working/nsp_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/bert_nsp_with_pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e55ab10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T16:56:59.911740Z",
     "iopub.status.busy": "2024-05-25T16:56:59.910949Z",
     "iopub.status.idle": "2024-05-25T17:06:26.307029Z",
     "shell.execute_reply": "2024-05-25T17:06:26.305941Z"
    },
    "papermill": {
     "duration": 572.515175,
     "end_time": "2024-05-25T17:06:26.309339",
     "exception": false,
     "start_time": "2024-05-25T16:56:53.794164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 16:57:05.389773: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-25 16:57:05.389825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-25 16:57:05.391371: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Output files will be saved in folder: /kaggle/working/output/bert_nsp_with_pretrain\r\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/output/bert_nsp_with_pretrain/pre_trained_model and are newly initialized: ['classifier.bias', 'classifier.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                   | 0/919 [00:00<?, ?it/s]/kaggle/working/bert_medical_records/load_dataset.py:105: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3614.)\r\n",
      "  inputs['labels'] = torch.LongTensor([label]).T\r\n",
      "/kaggle/working/bert_medical_records/load_dataset.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n",
      "  return {key: torch.tensor(val[0]) for key,val in inputs.items()}\r\n",
      "Epoch 0: 100%|████████████████████| 919/919 [08:30<00:00,  1.80it/s, loss=0.648]\r\n",
      "Average loss = 0.6476045846939087\r\n",
      "Evaluating: 100%|███████████████████| 230/230 [00:44<00:00,  5.23it/s, loss=143]\r\n",
      "1581\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\r\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\r\n",
      "result = {'acc': 0.688588850174216, 'f1': 0.8155790559711116, 'acc_and_f1': 0.7520839530726637}\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/bert_medical_records/run_glue.py \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --train_batch_size 10 \\\n",
    "      --num_epochs 1 \\\n",
    "      --model_input /kaggle/working/output/bert_nsp_with_pretrain \\\n",
    "      --input_file /kaggle/working/finetuning_dataset.txt \\\n",
    "      --output_dir /kaggle/working/output/bert_nsp_with_pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71559b77",
   "metadata": {
    "papermill": {
     "duration": 6.41477,
     "end_time": "2024-05-25T17:06:38.983944",
     "exception": false,
     "start_time": "2024-05-25T17:06:32.569174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create zip folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a89844aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:06:51.633369Z",
     "iopub.status.busy": "2024-05-25T17:06:51.632972Z",
     "iopub.status.idle": "2024-05-25T17:13:02.089874Z",
     "shell.execute_reply": "2024-05-25T17:13:02.088948Z"
    },
    "papermill": {
     "duration": 383.245717,
     "end_time": "2024-05-25T17:13:08.470839",
     "exception": false,
     "start_time": "2024-05-25T17:06:45.225122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/output.zip'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the path to the folder you want to zip\n",
    "folder_path = '/kaggle/working/output'\n",
    "\n",
    "# Specify the path for the resulting zip file\n",
    "zip_file_path = '/kaggle/working/output'\n",
    "\n",
    "# Create a zip file\n",
    "shutil.make_archive(zip_file_path, 'zip', folder_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4012665,
     "sourceId": 6982185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4466132,
     "sourceId": 7659768,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4490406,
     "sourceId": 7693946,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5061506,
     "sourceId": 8485127,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25168.425123,
   "end_time": "2024-05-25T17:13:15.021919",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-25T10:13:46.596796",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
