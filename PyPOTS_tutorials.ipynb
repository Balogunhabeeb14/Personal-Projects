{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Balogunhabeeb14/Personal-Projects/blob/main/PyPOTS_tutorials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 😎 Quick-start Tutorials for PyPOTS are Here!"
      ],
      "metadata": {
        "id": "DonG1Q5K8lFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Installation"
      ],
      "metadata": {
        "id": "nDZqwFN4899V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install pypots >=0.1\n",
        "! pip install pypots==0.1.1\n"
      ],
      "metadata": {
        "id": "xe-rPZzC9CoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97cab8b-f060-4802-8960-6852295c31e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypots==0.1.1\n",
            "  Downloading pypots-0.1.1-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.0/150.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pypots==0.1.1) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pypots==0.1.1) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pypots==0.1.1) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pypots==0.1.1) (2.0.1+cu118)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from pypots==0.1.1) (2.12.2)\n",
            "Requirement already satisfied: pandas<2.0.0 in /usr/local/lib/python3.10/dist-packages (from pypots==0.1.1) (1.5.3)\n",
            "Collecting pycorruptor (from pypots==0.1.1)\n",
            "  Downloading pycorruptor-0.0.4-py3-none-any.whl (17 kB)\n",
            "Collecting tsdb (from pypots==0.1.1)\n",
            "  Downloading tsdb-0.0.8-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from pypots==0.1.1) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0->pypots==0.1.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0->pypots==0.1.1) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots==0.1.1) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots==0.1.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots==0.1.1) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots==0.1.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots==0.1.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots==0.1.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->pypots==0.1.1) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->pypots==0.1.1) (16.0.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pypots==0.1.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pypots==0.1.1) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots==0.1.1) (0.40.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->pypots==0.1.1) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->pypots==0.1.1) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->pypots==0.1.1) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->pypots==0.1.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->pypots==0.1.1) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->pypots==0.1.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->pypots==0.1.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->pypots==0.1.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->pypots==0.1.1) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->pypots==0.1.1) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pypots==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->pypots==0.1.1) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->pypots==0.1.1) (3.2.2)\n",
            "Installing collected packages: tsdb, pycorruptor, pypots\n",
            "Successfully installed pycorruptor-0.0.4 pypots-0.1.1 tsdb-0.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📀 Preparing the **PhysioNet-2012** dataset for this tutorial"
      ],
      "metadata": {
        "id": "StrWh_It88lF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yASJSepF17za",
        "outputId": "17c9e2d2-1f2e-4a8c-b21c-c47a5f7609a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-22 12:24:35 [INFO]: Done. Have already set the random seed as 2204 for numpy and pytorch.\n",
            "2023-05-22 12:24:35 [INFO]: Loading the dataset physionet_2012 with TSDB (https://github.com/WenjieDu/Time_Series_Database)...\n",
            "2023-05-22 12:24:35 [INFO]: Starting preprocessing physionet_2012...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset physionet_2012 has already been downloaded. Processing directly...\n",
            "Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
            "Loaded successfully!\n",
            "dict_keys(['n_classes', 'n_steps', 'n_features', 'train_X', 'train_y', 'val_X', 'val_y', 'test_X', 'test_y', 'scaler', 'test_X_intact', 'test_X_indicating_mask', 'val_X_intact', 'val_X_indicating_mask'])\n"
          ]
        }
      ],
      "source": [
        "from pypots.data.generating import gene_physionet2012\n",
        "from pypots.utils.random import set_random_seed\n",
        "\n",
        "set_random_seed()\n",
        "\n",
        "# Load the PhysioNet-2012 dataset\n",
        "physionet2012_dataset = gene_physionet2012(artificially_missing_rate=0.1)\n",
        "\n",
        "# Take a look at the generated PhysioNet-2012 dataset, you'll find that everything has been prepared for you,\n",
        "# data splitting, normalization, additional artificially-missing values for evaluation, etc.\n",
        "print(physionet2012_dataset.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🌟 Imputation Models"
      ],
      "metadata": {
        "id": "lc3SoTqg9r2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the datasets for training, validating, and testing.\n",
        "\n",
        "dataset_for_training = {\n",
        "    \"X\": physionet2012_dataset['train_X'],\n",
        "}\n",
        "\n",
        "dataset_for_validating = {\n",
        "    \"X\": physionet2012_dataset['val_X'],\n",
        "    \"X_intact\": physionet2012_dataset['val_X_intact'],\n",
        "    \"indicating_mask\": physionet2012_dataset['val_X_indicating_mask'],\n",
        "}\n",
        "\n",
        "dataset_for_testing = {\n",
        "    \"X\": physionet2012_dataset['test_X'],\n",
        "}\n"
      ],
      "metadata": {
        "id": "Z11KSsFu-A9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **SAITS** for imputation"
      ],
      "metadata": {
        "id": "oVqyqt_S9z3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import SAITS\n",
        "from pypots.utils.metrics import cal_mae\n",
        "\n",
        "# initialize the model\n",
        "saits = SAITS(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_layers=2,\n",
        "    d_model=256,\n",
        "    d_inner=128,\n",
        "    n_heads=4,\n",
        "    d_k=64,\n",
        "    d_v=64,\n",
        "    dropout=0.1,\n",
        "    attn_dropout=0.1,\n",
        "    diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
        "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
        "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
        "    MIT_weight=1,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
        "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/saits\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "saits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "saits_imputation = saits.impute(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = cal_mae(\n",
        "    saits_imputation, physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
        "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcxUvLNE9rNL",
        "outputId": "c10f682d-0d06-4b6e-9985-39251d7fa34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-21 17:35:28 [INFO]: No given device, using default device: cuda\n",
            "2023-05-21 17:35:28 [INFO]: Model files will be saved to tutorial_results/imputation/saits/20230521_T173528\n",
            "2023-05-21 17:35:28 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits/20230521_T173528/tensorboard\n",
            "2023-05-21 17:35:28 [INFO]: Model initialized successfully with the number of trainable parameters: 1,378,358\n",
            "2023-05-21 17:35:35 [INFO]: epoch 0: training loss 0.7098, validating loss 0.3240\n",
            "2023-05-21 17:35:41 [INFO]: epoch 1: training loss 0.5091, validating loss 0.2987\n",
            "2023-05-21 17:35:48 [INFO]: epoch 2: training loss 0.4537, validating loss 0.2798\n",
            "2023-05-21 17:35:55 [INFO]: epoch 3: training loss 0.4150, validating loss 0.2640\n",
            "2023-05-21 17:36:01 [INFO]: epoch 4: training loss 0.3868, validating loss 0.2486\n",
            "2023-05-21 17:36:08 [INFO]: epoch 5: training loss 0.3665, validating loss 0.2466\n",
            "2023-05-21 17:36:16 [INFO]: epoch 6: training loss 0.3529, validating loss 0.2393\n",
            "2023-05-21 17:36:23 [INFO]: epoch 7: training loss 0.3426, validating loss 0.2364\n",
            "2023-05-21 17:36:29 [INFO]: epoch 8: training loss 0.3355, validating loss 0.2354\n",
            "2023-05-21 17:36:38 [INFO]: epoch 9: training loss 0.3293, validating loss 0.2312\n",
            "2023-05-21 17:36:38 [INFO]: Finished training.\n",
            "2023-05-21 17:36:38 [INFO]: Saved the model to tutorial_results/imputation/saits/20230521_T173528/SAITS.pypots.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mean absolute error: 0.2305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **Transformer** for imputation"
      ],
      "metadata": {
        "id": "15BwxZCW-OLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import Transformer\n",
        "from pypots.utils.metrics import cal_mae\n",
        "\n",
        "# initialize the model\n",
        "transformer = Transformer(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_layers=6,\n",
        "    d_model=512,\n",
        "    d_inner=256,\n",
        "    n_heads=4,\n",
        "    d_k=128,\n",
        "    d_v=128,\n",
        "    dropout=0.1,\n",
        "    attn_dropout=0,\n",
        "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
        "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
        "    MIT_weight=1,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
        "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/transformer\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "transformer.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "transformer_imputation = transformer.impute(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = cal_mae(\n",
        "    transformer_imputation,\n",
        "    physionet2012_dataset['test_X_intact'],\n",
        "    physionet2012_dataset['test_X_indicating_mask']\n",
        ")\n",
        "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRUFFj11-NWj",
        "outputId": "e2c07705-26d7-456d-aeb1-4ae7cd69ba39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-21 17:32:12 [INFO]: No given device, using default device: cuda\n",
            "2023-05-21 17:32:12 [INFO]: Model files will be saved to tutorial_results/imputation/transformer/20230521_T173212\n",
            "2023-05-21 17:32:12 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/transformer/20230521_T173212/tensorboard\n",
            "2023-05-21 17:32:19 [INFO]: Model initialized successfully with the number of trainable parameters: 7,938,597\n",
            "2023-05-21 17:32:31 [INFO]: epoch 0: training loss 0.8080, validating loss 0.5966\n",
            "2023-05-21 17:32:40 [INFO]: epoch 1: training loss 0.6327, validating loss 0.5566\n",
            "2023-05-21 17:32:49 [INFO]: epoch 2: training loss 0.5875, validating loss 0.5367\n",
            "2023-05-21 17:32:59 [INFO]: epoch 3: training loss 0.5696, validating loss 0.5303\n",
            "2023-05-21 17:33:08 [INFO]: epoch 4: training loss 0.5617, validating loss 0.5346\n",
            "2023-05-21 17:33:17 [INFO]: epoch 5: training loss 0.5543, validating loss 0.5240\n",
            "2023-05-21 17:33:27 [INFO]: epoch 6: training loss 0.5510, validating loss 0.5108\n",
            "2023-05-21 17:33:37 [INFO]: epoch 7: training loss 0.5490, validating loss 0.5156\n",
            "2023-05-21 17:33:46 [INFO]: epoch 8: training loss 0.5463, validating loss 0.5137\n",
            "2023-05-21 17:33:56 [INFO]: epoch 9: training loss 0.5444, validating loss 0.5054\n",
            "2023-05-21 17:33:56 [INFO]: Finished training.\n",
            "2023-05-21 17:33:56 [INFO]: Saved the model to tutorial_results/imputation/transformer/20230521_T173212/Transformer.pypots.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mean absolute error: 0.5056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **BRITS** for imputation"
      ],
      "metadata": {
        "id": "GGL2QaXl-nm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import BRITS\n",
        "from pypots.utils.metrics import cal_mae\n",
        "\n",
        "# initialize the model\n",
        "# initialize the model\n",
        "brits = BRITS(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    rnn_hidden_size=128,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
        "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/brits\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "brits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "brits_imputation = brits.impute(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = cal_mae(\n",
        "    brits_imputation,\n",
        "    physionet2012_dataset['test_X_intact'],\n",
        "    physionet2012_dataset['test_X_indicating_mask']\n",
        ")\n",
        "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
      ],
      "metadata": {
        "id": "Tgbgou-_-qMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b76ce9-0750-4956-f17f-8155ba7f250b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-21 17:37:39 [INFO]: No given device, using default device: cuda\n",
            "2023-05-21 17:37:39 [INFO]: Model files will be saved to tutorial_results/imputation/brits/20230521_T173739\n",
            "2023-05-21 17:37:39 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/brits/20230521_T173739/tensorboard\n",
            "2023-05-21 17:37:39 [INFO]: Model initialized successfully with the number of trainable parameters: 239,344\n",
            "2023-05-21 17:39:03 [INFO]: epoch 0: training loss 0.9475, validating loss 0.3534\n",
            "2023-05-21 17:40:02 [INFO]: epoch 1: training loss 0.7369, validating loss 0.3107\n",
            "2023-05-21 17:41:01 [INFO]: epoch 2: training loss 0.6845, validating loss 0.2903\n",
            "2023-05-21 17:42:00 [INFO]: epoch 3: training loss 0.6596, validating loss 0.2800\n",
            "2023-05-21 17:42:59 [INFO]: epoch 4: training loss 0.6443, validating loss 0.2738\n",
            "2023-05-21 17:43:57 [INFO]: epoch 5: training loss 0.6329, validating loss 0.2691\n",
            "2023-05-21 17:44:56 [INFO]: epoch 6: training loss 0.6238, validating loss 0.2660\n",
            "2023-05-21 17:45:55 [INFO]: epoch 7: training loss 0.6158, validating loss 0.2636\n",
            "2023-05-21 17:46:54 [INFO]: epoch 8: training loss 0.6088, validating loss 0.2614\n",
            "2023-05-21 17:47:54 [INFO]: epoch 9: training loss 0.6032, validating loss 0.2598\n",
            "2023-05-21 17:47:54 [INFO]: Finished training.\n",
            "2023-05-21 17:47:54 [INFO]: Saved the model to tutorial_results/imputation/brits/20230521_T173739/BRITS.pypots.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mean absolute error: 0.2555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **M-RNN** for imputation"
      ],
      "metadata": {
        "id": "uPw7U1AgYmbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import MRNN\n",
        "from pypots.utils.metrics import cal_mae\n",
        "\n",
        "# initialize the model\n",
        "# initialize the model\n",
        "mrnn = MRNN(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    rnn_hidden_size=128,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
        "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/mrnn\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "mrnn.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "mrnn_imputation = mrnn.impute(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = cal_mae(\n",
        "    mrnn_imputation,\n",
        "    physionet2012_dataset['test_X_intact'],\n",
        "    physionet2012_dataset['test_X_indicating_mask']\n",
        ")\n",
        "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gruCE389YpF-",
        "outputId": "e1c4c932-7475-4fed-80cd-a9b7d6627c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-21 17:48:06 [INFO]: No given device, using default device: cuda\n",
            "2023-05-21 17:48:06 [INFO]: Model files will be saved to tutorial_results/imputation/mrnn/20230521_T174806\n",
            "2023-05-21 17:48:06 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/mrnn/20230521_T174806/tensorboard\n",
            "2023-05-21 17:48:06 [INFO]: Model initialized successfully with the number of trainable parameters: 265,939\n",
            "2023-05-21 17:48:54 [INFO]: epoch 0: training loss 1.0076, validating loss 0.6060\n",
            "2023-05-21 17:49:18 [INFO]: epoch 1: training loss 0.4206, validating loss 0.6812\n",
            "2023-05-21 17:49:43 [INFO]: epoch 2: training loss 0.3212, validating loss 0.7078\n",
            "2023-05-21 17:50:07 [INFO]: epoch 3: training loss 0.2673, validating loss 0.7258\n",
            "2023-05-21 17:50:07 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2023-05-21 17:50:07 [INFO]: Finished training.\n",
            "2023-05-21 17:50:07 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20230521_T174806/MRNN.pypots.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mean absolute error: 0.7208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **LOCF** for imputation"
      ],
      "metadata": {
        "id": "14XhW8yN-3gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.imputation import LOCF\n",
        "from pypots.utils.metrics import cal_mae\n",
        "\n",
        "# initialize the model\n",
        "locf = LOCF(\n",
        "    nan=0  # set the value used to impute data missing at the beginning of the sequence, those cannot use LOCF mechanism to impute\n",
        ")\n",
        "\n",
        "# LOCF doesn't need to be trained, just call the impute() function\n",
        "locf.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "locf_imputation = locf.impute(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = cal_mae(\n",
        "    locf_imputation,\n",
        "    physionet2012_dataset['test_X_intact'],\n",
        "    physionet2012_dataset['test_X_indicating_mask']\n",
        ")\n",
        "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
      ],
      "metadata": {
        "id": "qZS5aP2j-5TE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03add67-0ae9-4a49-9098-f79a8332d963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-21 17:50:35 [INFO]: saving_path not given. Model files and tensorboard file will not be saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mean absolute error: 0.4079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypots/imputation/locf/model.py:47: UserWarning: LOCF (Last Observed Carried Forward) imputation class has no parameter to train. Please run func impute(X) directly.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🌟 Clustering Models"
      ],
      "metadata": {
        "id": "H4bU3eZY_-01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the datasets for training, validating, and testing.\n",
        "import numpy as np\n",
        "\n",
        "# don't need validation set\n",
        "dataset_for_training = {\n",
        "    \"X\": np.concatenate([physionet2012_dataset['train_X'], physionet2012_dataset['val_X']], axis=0),\n",
        "    \"y\": np.concatenate([physionet2012_dataset['train_y'], physionet2012_dataset['val_y']], axis=0),\n",
        "}\n",
        "\n",
        "dataset_for_testing = {\n",
        "    \"X\": physionet2012_dataset['test_X'],\n",
        "    \"y\": physionet2012_dataset['test_y'],\n",
        "}\n"
      ],
      "metadata": {
        "id": "udnvIVuUADBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **CRLI** for clustering"
      ],
      "metadata": {
        "id": "QvyVryEwAEEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.clustering import CRLI\n",
        "from pypots.utils.metrics import cal_rand_index, cal_cluster_purity\n",
        "\n",
        "# initialize the model\n",
        "crli = CRLI(\n",
        "    n_steps=physionet2012_dataset[\"n_steps\"],\n",
        "    n_features=physionet2012_dataset[\"n_features\"],\n",
        "    n_clusters=physionet2012_dataset[\"n_classes\"],\n",
        "    n_generator_layers=2,\n",
        "    rnn_hidden_size=256,\n",
        "    rnn_cell_type=\"GRU\",\n",
        "    decoder_fcn_output_dims=[256, 128],  # the output dimensions of layers in the decoder FCN.\n",
        "    # Here means there are 3 layers. Leave it to default as None will results in\n",
        "    # the FCN haveing only one layer.\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    G_optimizer=Adam(lr=1e-3),\n",
        "    D_optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
        "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/clustering/crli\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "crli.fit(train_set=dataset_for_training)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "crli_prediction = crli.cluster(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "RI = cal_rand_index(crli_prediction, dataset_for_testing[\"y\"])\n",
        "CP = cal_cluster_purity(crli_prediction, dataset_for_testing[\"y\"])\n",
        "print(\n",
        "    \"Testing clustering metrics: \\n\"\n",
        "    f'RI: {RI}, \\n'\n",
        "    f'CP: {CP}\\n'\n",
        ")\n"
      ],
      "metadata": {
        "id": "SjdHa-fiAJbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a6176a-e710-4c79-d587-82d8db366e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-21 17:52:10 [INFO]: No given device, using default device: cuda\n",
            "2023-05-21 17:52:10 [INFO]: Model files will be saved to tutorial_results/clustering/crli/20230521_T175210\n",
            "2023-05-21 17:52:10 [INFO]: Tensorboard file will be saved to tutorial_results/clustering/crli/20230521_T175210/tensorboard\n",
            "2023-05-21 17:52:10 [INFO]: Model initialized successfully with the number of trainable parameters: 1,546,820\n",
            "2023-05-21 17:53:37 [INFO]: epoch 0: training loss_generator 3.3941, train loss_discriminator 0.3881\n",
            "2023-05-21 17:55:01 [INFO]: epoch 1: training loss_generator 3.4165, train loss_discriminator 0.3679\n",
            "2023-05-21 17:56:25 [INFO]: epoch 2: training loss_generator 3.4143, train loss_discriminator 0.3492\n",
            "2023-05-21 17:57:48 [INFO]: epoch 3: training loss_generator 9.8183, train loss_discriminator 0.3325\n",
            "2023-05-21 17:57:48 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2023-05-21 17:57:48 [INFO]: Finished training.\n",
            "2023-05-21 17:57:48 [INFO]: Saved the model to tutorial_results/clustering/crli/20230521_T175210/CRLI.pypots.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing clustering metrics: \n",
            "RI: 0.4999754697542069, \n",
            "CP: 0.8586321934945789\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **VaDER** for clustering"
      ],
      "metadata": {
        "id": "UhoovboEATo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.clustering import VaDER\n",
        "from pypots.utils.metrics import cal_rand_index, cal_cluster_purity\n",
        "\n",
        "# initialize the model\n",
        "vader = VaDER(\n",
        "    n_steps=physionet2012_dataset[\"n_steps\"],\n",
        "    n_features=physionet2012_dataset[\"n_features\"],\n",
        "    n_clusters=physionet2012_dataset[\"n_classes\"],\n",
        "    rnn_hidden_size=128,\n",
        "    d_mu_stddev=2,\n",
        "    pretrain_epochs=20,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
        "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/clustering/vader\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "vader.fit(train_set=dataset_for_training)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "vader_prediction = vader.cluster(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "RI = cal_rand_index(vader_prediction, dataset_for_testing[\"y\"])\n",
        "CP = cal_cluster_purity(vader_prediction, dataset_for_testing[\"y\"])\n",
        "print(\n",
        "    \"Testing clustering metrics: \\n\"\n",
        "    f'RI: {RI}, \\n'\n",
        "    f'CP: {CP}\\n'\n",
        ")\n"
      ],
      "metadata": {
        "id": "HN9kmDtoAWuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e596326-86d6-4447-b349-ec59700b6d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-21 18:53:57 [INFO]: No given device, using default device: cuda\n",
            "2023-05-21 18:53:57 [INFO]: Model files will be saved to tutorial_results/clustering/vader/20230521_T185357\n",
            "2023-05-21 18:53:57 [INFO]: Tensorboard file will be saved to tutorial_results/clustering/vader/20230521_T185357/tensorboard\n",
            "2023-05-21 18:53:57 [INFO]: Model initialized successfully with the number of trainable parameters: 293,644\n",
            "2023-05-21 19:08:50 [INFO]: epoch 0: training loss 1.5763\n",
            "2023-05-21 19:09:32 [INFO]: epoch 1: training loss 1.0839\n",
            "2023-05-21 19:10:09 [INFO]: epoch 2: training loss 1.0469\n",
            "2023-05-21 19:10:46 [INFO]: epoch 3: training loss 1.0383\n",
            "2023-05-21 19:11:23 [INFO]: epoch 4: training loss 1.0328\n",
            "2023-05-21 19:12:01 [INFO]: epoch 5: training loss 1.0359\n",
            "2023-05-21 19:12:41 [INFO]: epoch 6: training loss 1.0371\n",
            "2023-05-21 19:13:18 [INFO]: epoch 7: training loss 1.0402\n",
            "2023-05-21 19:13:18 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2023-05-21 19:13:18 [INFO]: Finished training.\n",
            "2023-05-21 19:13:18 [INFO]: Saved the model to tutorial_results/clustering/vader/20230521_T185357/VaDER.pypots.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing clustering metrics: \n",
            "RI: 0.7500013048003081, \n",
            "CP: 0.853628023352794\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🌟 Forecasting Models"
      ],
      "metadata": {
        "id": "UoR-KkgtAiZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the datasets for training, validating, and testing.\n",
        "\n",
        "dataset_for_training = {\n",
        "    \"X\": physionet2012_dataset['train_X'],\n",
        "}\n",
        "\n",
        "dataset_for_validating = {\n",
        "    \"X\": physionet2012_dataset['val_X'],\n",
        "    \"X_intact\": physionet2012_dataset['val_X_intact'],\n",
        "    \"indicating_mask\": physionet2012_dataset['val_X_indicating_mask'],\n",
        "}\n",
        "\n",
        "dataset_for_testing = {\n",
        "    \"X\": physionet2012_dataset['test_X'][:, :36],  # we only take the first 36 steps for model input,\n",
        "    # and let the model to forecast the left 12 steps\n",
        "}\n"
      ],
      "metadata": {
        "id": "dvsTR8ARAh5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **BTTF** for forecasting"
      ],
      "metadata": {
        "id": "jVIB7HhdAn2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.forecasting import BTTF\n",
        "from pypots.utils.metrics import cal_mae\n",
        "\n",
        "# initialize the model\n",
        "bttf = BTTF(\n",
        "    36,\n",
        "    physionet2012_dataset[\"n_features\"],\n",
        "    pred_step=12,\n",
        "    rank=10,\n",
        "    time_lags=[1, 2, 3, 10, 10 + 1, 10 + 2, 20, 20 + 1, 20 + 2],\n",
        "    burn_iter=5,\n",
        "    gibbs_iter=5,\n",
        "    multi_step=1,\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "bttf.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "# BTTF does not need to run func fits().\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "bttf_forecasting_results = bttf.forecast(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = cal_mae(\n",
        "    bttf_forecasting_results,\n",
        "    np.nan_to_num(physionet2012_dataset['test_X'][:, 36:]),\n",
        "    (~np.isnan(physionet2012_dataset['test_X'][:, 36:])).astype(int),\n",
        ")\n",
        "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
      ],
      "metadata": {
        "id": "brxb6kAoAq6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3497650f-2263-4e20-c19c-c3c8d814b10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-21 18:14:45 [INFO]: No given device, using default device: cuda\n",
            "2023-05-21 18:14:45 [INFO]: saving_path not given. Model files and tensorboard file will not be saved.\n",
            "/usr/local/lib/python3.10/dist-packages/pypots/forecasting/bttf/model.py:352: UserWarning: Please run func forecast(X) directly.\n",
            "  warnings.warn(\"Please run func forecast(X) directly.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mean absolute error: 1.2239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🌟 Classification Models"
      ],
      "metadata": {
        "id": "PnNGplDi_H09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the datasets for training, validating, and testing.\n",
        "\n",
        "dataset_for_training = {\n",
        "    \"X\": physionet2012_dataset['train_X'],\n",
        "    \"y\": physionet2012_dataset['train_y'],\n",
        "}\n",
        "\n",
        "dataset_for_validating = {\n",
        "    \"X\": physionet2012_dataset['val_X'],\n",
        "    \"y\": physionet2012_dataset['val_y'],\n",
        "}\n",
        "\n",
        "dataset_for_testing = {\n",
        "    \"X\": physionet2012_dataset['test_X'],\n",
        "    \"y\": physionet2012_dataset['test_y'],\n",
        "}"
      ],
      "metadata": {
        "id": "Hm5IMCa1-8WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **BRITS** for classification"
      ],
      "metadata": {
        "id": "qlluYv6q_jMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.classification import BRITS\n",
        "from pypots.utils.metrics import cal_binary_classification_metrics\n",
        "\n",
        "# initialize the model\n",
        "from pypots.optim import Adam\n",
        "from pypots.classification import BRITS\n",
        "\n",
        "# initialize the model\n",
        "brits = BRITS(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_classes=physionet2012_dataset[\"n_classes\"],\n",
        "    rnn_hidden_size=256,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
        "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/classification/brits\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "brits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "brits_prediction = brits.classify(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "metrics = cal_binary_classification_metrics(brits_prediction, dataset_for_testing[\"y\"])\n",
        "print(\"Testing classification metrics: \\n\"\n",
        "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
        "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
        "    f'F1: {metrics[\"f1\"]},\\n'\n",
        "    f'Precision: {metrics[\"precision\"]},\\n'\n",
        "    f'Recall: {metrics[\"recall\"]},\\n'\n",
        ")"
      ],
      "metadata": {
        "id": "FvxZl33v_jCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88850da0-deb1-4ea7-a2d8-db373f68a64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-21 18:42:05 [ERROR]: cannot import name 'ObservationPropagation' from 'pypots.classification.raindrop.modules' (/usr/local/lib/python3.10/dist-packages/pypots/classification/raindrop/modules.py)\n",
            "torch_geometric is missing, please install it with 'pip install torch_geometric' or 'conda install -c pyg pyg'\n",
            "2023-05-21 18:42:05 [INFO]: No given device, using default device: cuda\n",
            "2023-05-21 18:42:05 [INFO]: Model files will be saved to tutorial_results/classification/brits/20230521_T184205\n",
            "2023-05-21 18:42:05 [INFO]: Tensorboard file will be saved to tutorial_results/classification/brits/20230521_T184205/tensorboard\n",
            "2023-05-21 18:42:08 [INFO]: Model initialized successfully with the number of trainable parameters: 730,612\n",
            "2023-05-21 18:43:31 [INFO]: epoch 0: training loss 0.9122, validating loss 0.8039\n",
            "2023-05-21 18:44:32 [INFO]: epoch 1: training loss 0.7784, validating loss 0.7490\n",
            "2023-05-21 18:45:31 [INFO]: epoch 2: training loss 0.7265, validating loss 0.7239\n",
            "2023-05-21 18:46:31 [INFO]: epoch 3: training loss 0.7012, validating loss 0.7163\n",
            "2023-05-21 18:47:31 [INFO]: epoch 4: training loss 0.6852, validating loss 0.7230\n",
            "2023-05-21 18:48:31 [INFO]: epoch 5: training loss 0.6738, validating loss 0.7154\n",
            "2023-05-21 18:49:31 [INFO]: epoch 6: training loss 0.6592, validating loss 0.7197\n",
            "2023-05-21 18:50:31 [INFO]: epoch 7: training loss 0.6417, validating loss 0.7188\n",
            "2023-05-21 18:51:30 [INFO]: epoch 8: training loss 0.6279, validating loss 0.7165\n",
            "2023-05-21 18:51:30 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2023-05-21 18:51:30 [INFO]: Finished training.\n",
            "2023-05-21 18:51:30 [INFO]: Saved the model to tutorial_results/classification/brits/20230521_T184205/BRITS.pypots.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing classification metrics: \n",
            "ROC_AUC: 0.8344878266715101, \n",
            "PR_AUC: 0.46329618945884665,\n",
            "F1: 0.31865828092243187,\n",
            "Precision: 0.6031746031746031,\n",
            "Recall: 0.21652421652421652,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **GRUD** for classification"
      ],
      "metadata": {
        "id": "msmIoNGH_wQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.classification import GRUD\n",
        "from pypots.utils.metrics import cal_binary_classification_metrics\n",
        "\n",
        "# initialize the model\n",
        "from pypots.optim import Adam\n",
        "from pypots.classification import BRITS\n",
        "\n",
        "# initialize the model\n",
        "grud = GRUD(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_classes=physionet2012_dataset[\"n_classes\"],\n",
        "    rnn_hidden_size=32,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
        "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/classification/grud\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "grud.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "grud_prediction = grud.classify(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "metrics = cal_binary_classification_metrics(grud_prediction, dataset_for_testing[\"y\"])\n",
        "print(\"Testing classification metrics: \\n\"\n",
        "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
        "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
        "    f'F1: {metrics[\"f1\"]},\\n'\n",
        "    f'Precision: {metrics[\"precision\"]},\\n'\n",
        "    f'Recall: {metrics[\"recall\"]},\\n'\n",
        ")"
      ],
      "metadata": {
        "id": "j-hLkYbR_vru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d208efda-288f-41f2-a62a-e5cdf090e87f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-22 12:15:28 [ERROR]: No module named 'torch_geometric'\n",
            "torch_geometric is missing, please install it with 'pip install torch_geometric' or 'conda install -c pyg pyg'\n",
            "2023-05-22 12:15:28 [ERROR]: name 'MessagePassing' is not defined\n",
            "Note torch_geometric is missing, please install it with 'pip install torch_geometric' or 'conda install -c pyg pyg'\n",
            "2023-05-22 12:15:28 [INFO]: No given device, using default device: cuda\n",
            "2023-05-22 12:15:28 [INFO]: Model files will be saved to tutorial_results/classification/grud/20230522_T121528\n",
            "2023-05-22 12:15:28 [INFO]: Tensorboard file will be saved to tutorial_results/classification/grud/20230522_T121528/tensorboard\n",
            "2023-05-22 12:15:35 [INFO]: Model initialized successfully with the number of trainable parameters: 16,128\n",
            "2023-05-22 12:15:35 [INFO]: saving_path not given. Model files and tensorboard file will not be saved.\n",
            "2023-05-22 12:15:46 [INFO]: saving_path not given. Model files and tensorboard file will not be saved.\n",
            "2023-05-22 12:16:06 [INFO]: epoch 0: training loss 0.3862, validating loss 0.3560\n",
            "2023-05-22 12:16:21 [INFO]: epoch 1: training loss 0.3298, validating loss 0.3436\n",
            "2023-05-22 12:16:36 [INFO]: epoch 2: training loss 0.3072, validating loss 0.3512\n",
            "2023-05-22 12:16:51 [INFO]: epoch 3: training loss 0.2983, validating loss 0.3411\n",
            "2023-05-22 12:17:06 [INFO]: epoch 4: training loss 0.2912, validating loss 0.3321\n",
            "2023-05-22 12:17:22 [INFO]: epoch 5: training loss 0.2845, validating loss 0.3413\n",
            "2023-05-22 12:17:37 [INFO]: epoch 6: training loss 0.2786, validating loss 0.3460\n",
            "2023-05-22 12:17:53 [INFO]: epoch 7: training loss 0.2710, validating loss 0.3439\n",
            "2023-05-22 12:17:53 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2023-05-22 12:17:53 [INFO]: Finished training.\n",
            "2023-05-22 12:17:53 [INFO]: Saved the model to tutorial_results/classification/grud/20230522_T121528/GRUD.pypots.\n",
            "2023-05-22 12:17:53 [INFO]: saving_path not given. Model files and tensorboard file will not be saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing classification metrics: \n",
            "ROC_AUC: 0.8124554156025495, \n",
            "PR_AUC: 0.4419410911679675,\n",
            "F1: 0.39626168224299063,\n",
            "Precision: 0.53,\n",
            "Recall: 0.3164179104477612,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 An exmaple of **Raindrop** for classification"
      ],
      "metadata": {
        "id": "Y1uXdzhY_Ypx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vao00NqZTPm",
        "outputId": "204fe708-e4ae-456a-a7d7-f05505b469ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install necessary dependencies for Raindrop\n",
        "! pip install torch-geometric torch-scatter torch-sparse -f \"https://data.pyg.org/whl/torch-2.0.0+cu118.html\"\n"
      ],
      "metadata": {
        "id": "4iA49L0bF3vL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8317fe48-1f33-4226-dd87-9b0f5d7019fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.3.1-py3-none-any.whl\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1 torch-scatter-2.1.1+pt20cu118 torch-sparse-0.6.17+pt20cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.classification import Raindrop\n",
        "from pypots.utils.metrics import cal_binary_classification_metrics\n",
        "\n",
        "# initialize the model\n",
        "raindrop = Raindrop(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_classes=physionet2012_dataset[\"n_classes\"],\n",
        "    n_layers=2,\n",
        "    d_model=physionet2012_dataset[\"n_features\"] * 4,\n",
        "    d_inner=256,\n",
        "    n_heads=2,\n",
        "    dropout=0.3,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
        "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/classification/raindrop\",\n",
        "    model_saving_strategy=\"best\", # only save the best model after training finished.\n",
        "                                  # You can also set it as \"better\" to save models performing better ever during training.\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "raindrop.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "raindrop_prediction = raindrop.classify(dataset_for_testing)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "metrics = cal_binary_classification_metrics(raindrop_prediction, dataset_for_testing[\"y\"])\n",
        "print(\"Testing classification metrics: \\n\"\n",
        "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
        "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
        "    f'F1: {metrics[\"f1\"]},\\n'\n",
        "    f'Precision: {metrics[\"precision\"]},\\n'\n",
        "    f'Recall: {metrics[\"recall\"]},\\n'\n",
        ")"
      ],
      "metadata": {
        "id": "ShuLMQ0L-9mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0c8c87-8a16-40c9-8302-e6d8dcca5888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-22 12:25:24 [INFO]: No given device, using default device: cuda\n",
            "2023-05-22 12:25:24 [INFO]: Model files will be saved to tutorial_results/classification/raindrop/20230522_T122524\n",
            "2023-05-22 12:25:24 [INFO]: Tensorboard file will be saved to tutorial_results/classification/raindrop/20230522_T122524/tensorboard\n",
            "2023-05-22 12:25:26 [INFO]: Model initialized successfully with the number of trainable parameters: 1,415,006\n",
            "2023-05-22 12:25:26 [INFO]: saving_path not given. Model files and tensorboard file will not be saved.\n",
            "2023-05-22 12:25:37 [INFO]: saving_path not given. Model files and tensorboard file will not be saved.\n",
            "2023-05-22 12:26:05 [INFO]: epoch 0: training loss 0.3798, validating loss 0.3416\n",
            "2023-05-22 12:26:30 [INFO]: epoch 1: training loss 0.3387, validating loss 0.3300\n",
            "2023-05-22 12:26:55 [INFO]: epoch 2: training loss 0.3194, validating loss 0.3364\n",
            "2023-05-22 12:27:21 [INFO]: epoch 3: training loss 0.3095, validating loss 0.3104\n",
            "2023-05-22 12:27:47 [INFO]: epoch 4: training loss 0.3063, validating loss 0.3310\n",
            "2023-05-22 12:28:12 [INFO]: epoch 5: training loss 0.2994, validating loss 0.3263\n",
            "2023-05-22 12:28:38 [INFO]: epoch 6: training loss 0.2929, validating loss 0.3196\n",
            "2023-05-22 12:28:38 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2023-05-22 12:28:38 [INFO]: Finished training.\n",
            "2023-05-22 12:28:38 [INFO]: Saved the model to tutorial_results/classification/raindrop/20230522_T122524/Raindrop.pypots.\n",
            "2023-05-22 12:28:38 [INFO]: saving_path not given. Model files and tensorboard file will not be saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing classification metrics: \n",
            "ROC_AUC: 0.8478756743219553, \n",
            "PR_AUC: 0.5212918602764244,\n",
            "F1: 0.2418604651162791,\n",
            "Precision: 0.7761194029850746,\n",
            "Recall: 0.14325068870523416,\n",
            "\n"
          ]
        }
      ]
    }
  ]
}